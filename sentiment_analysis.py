# -*- coding: utf-8 -*-
"""Sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QJRq-2rlJ6ECAXUiiJqDZyL-j509x2H-
"""

from google.colab import drive

drive.mount('/content/gdrive/',force_remount=True)

path = '/content/gdrive/My Drive/sentiment/'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from gensim.models import Word2Vec

data = pd.read_csv(path+'airline_sentiment_analysis.csv')

data.head()

X = data['text'].values

data['airline_sentiment'].values [ data['airline_sentiment']=='positive'] = 1

data['airline_sentiment'].values [ data['airline_sentiment']=='negative'] = 0

Y = data['airline_sentiment'].values

Y  = np.array(Y,dtype='int32')


def word_to_num(dat):
  vocab = []
  for i in range(len(dat)):
    dat[i] = '<BOS> '+ dat[i] +' <EOS>'
    vocab = list(set( vocab + list(set(dat[i].split())) ) )  
  vocab+= ' '    
  temp1 = []  
  for i in dat:
    temp = []
    for j in i.split():
      temp.append( vocab.index(j))
    temp1.append(temp)

  return np.array(temp1),np.array(vocab)

X_data,vocab = word_to_num(X)


X_train = tf.keras.preprocessing.sequence.pad_sequences(X_data,padding='post',truncating='post',value=np.where(vocab==' ') )



def create_embed(vocabs):
     word2vec = Word2Vec(vocabs,size=300)
     embeddings = np.random.randn(len(vocabs),300)
     for i in range(len(vocabs)):
       if vocabs[i] in word2vec.wv.vocab:
         embeddings[i] = word2vec.wv.word_vec(vocabs[i])
     return embeddings

embeddings = create_embed(vocab)



def stack_LSTM_model(dt1,embeddings=embeddings,batch_size=16):
  layer2_1 = tf.keras.layers.Embedding(embeddings.shape[0],embeddings.shape[1],weights=[embeddings],batch_input_shape=[batch_size,None],trainable =False)

  layer2 = tf.keras.layers.LSTM(512, return_sequences=True, recurrent_initializer='glorot_uniform',recurrent_activation='sigmoid',stateful=True)
  layer3 = tf.keras.layers.LSTM(512, return_sequences=True, recurrent_initializer='glorot_uniform',recurrent_activation='sigmoid',stateful=True)
  layer4 = tf.keras.layers.Dense(150,activation = 'sigmoid')
  layer5 = tf.keras.layers.Dense(1,activation = 'sigmoid')
  
  layer1 = tf.keras.Input(shape=(None,),batch_size=batch_size)

  out1 = layer2_1(layer1)
  out1 = layer2(out1)
  out2 = layer3(out1)

  out3 = tf.keras.layers.GlobalMaxPooling1D()(out2)

  out3 = layer4(out3)

  out4 = layer5(out3)
  return tf.keras.models.Model(inputs=layer1,outputs=out4)

model = stack_LSTM_model(X_train[:150])

model.summary()

model.compile('adam','binary_crossentropy','accuracy')

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_train,Y, test_size=0.20, random_state=42)



hist = model.fit(x=X_train,y=y_train,batch_size=16,epochs=10)

model.evaluate(X_test[:2304],y_test[:2304],batch_size=16)

model.save_weights(path+'Stack_LSTM_model.h5')

def LSTM_model(dt1,embeddings=embeddings,batch_size=16):
  layer2_1 = tf.keras.layers.Embedding(embeddings.shape[0],embeddings.shape[1],weights=[embeddings],batch_input_shape=[batch_size,None],trainable =False)

  layer2 = tf.keras.layers.LSTM(512, return_sequences=True, recurrent_initializer='glorot_uniform',recurrent_activation='sigmoid',stateful=True)
  
  layer4 = tf.keras.layers.Dense(150,activation = 'sigmoid')
  layer5 = tf.keras.layers.Dense(1,activation = 'sigmoid')
  
  layer1 = tf.keras.Input(shape=(None,),batch_size=batch_size)

  out1 = layer2_1(layer1)
  out2 = layer2(out1)

  out3 = tf.keras.layers.GlobalMaxPooling1D()(out2)

  out3 = layer4(out3)

  out4 = layer5(out3)
  return tf.keras.models.Model(inputs=layer1,outputs=out4)

model = LSTM_model(X_train[:150])

model.compile('adam','binary_crossentropy','accuracy')

hist = model.fit(x=X_train,y=y_train,batch_size=16,epochs=10)

model.evaluate(X_test[:2304],y_test[:2304],batch_size=16)

model.save_weights(path+'LSTM_model.h5')